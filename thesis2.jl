if something(findfirst(isequal("./"),LOAD_PATH),0) == 0
    push!(LOAD_PATH, "./")
end
using Distributed
using Distributions
using ForwardDiff
using JuMP
using NLopt
using GR
using DataFrames
using MultivariateStats
using Flux
using DelimitedFiles
using Statistics
using Parameters
using Simulator

#Basis functions
Ï•(x) = [1, 1e-5*v(x),
        2.5e-9*(q(x)+h(x))*v(x),
        2.5e-3*s(x),
        1e-2*(s(x)-p(x)),
        #0.5e-3*(q(x)-h(x)),
        1e-5*q(x)*h(x),
        0.5e-3*(q(x)+h(x)),
        1e-7*(q(x)+h(x))^2,
        1e-7*(q(x)^2 + h(x)^2)]
Ï•(X::Array{Array{Float64,1},1}) = permutedims(Flux.batch(Ï•.(X)))

#Function approximations
V(Î¸,x) =  sum(Î¸ .* Ï•(x))
Î´áµƒ(Î¸,x) = sum(Î¸ .* Ï•(x))
Î´áµ‡(Î¸,x) = sum(Î¸ .* Ï•(x))
Î¾(Î¸,x) =  sum(Î¸ .* Ï•(x))

function get_param(X)
    x = X[1:5]
    Î¸ = X[6:length(X)]
    x,Î¸
end

function get_x(X)
    X[1:5]
end

#Value funcion as function of one master argument
#with both the state x and the parameters Î¸áµ›
function V(X)
    x,Î¸ = get_param(X)
    V(Î¸,x)
end

#Reward function partial derivatives
âˆ‡R(x) = ForwardDiff.gradient(R,x)
âˆ‡Â²R(x) = ForwardDiff.hessian(R,x)
âˆ‚Ry(x) = âˆ‡R(x)[1]
âˆ‚Rs(x) = âˆ‡R(x)[2]
âˆ‚Rp(x) = âˆ‡R(x)[3]
âˆ‚Rq(x) = âˆ‡R(x)[4]
âˆ‚Rh(x) = âˆ‡R(x)[5]
âˆ‚âˆ‚Rss(x) = âˆ‡Â²R(x)[2,2]
âˆ‚âˆ‚Rpp(x) = âˆ‡Â²R(x)[3,3]
âˆ‚âˆ‚Rsp(x) = âˆ‡Â²R(x)[2,3]

#Value function partial derivatives
âˆ‡V(Î¸,x) = ForwardDiff.gradient(V,[x;Î¸])
âˆ‡Vx(Î¸,x) = âˆ‡V(Î¸,x)[1:5]
âˆ‡Â²V(Î¸,x) = ForwardDiff.hessian(V,[x;Î¸])
âˆ‡Â²Vx(Î¸,x) = âˆ‡Â²V(Î¸,x)[1:5,1:5]
âˆ‚Vy(Î¸,x) = âˆ‡V(Î¸,x)[1]
âˆ‚Vs(Î¸,x) = âˆ‡V(Î¸,x)[2]
âˆ‚Vp(Î¸,x) = âˆ‡V(Î¸,x)[3]
âˆ‚Vq(Î¸,x) = âˆ‡V(Î¸,x)[4]
âˆ‚Vh(Î¸,x) = âˆ‡V(Î¸,x)[5]
âˆ‚âˆ‚Vss(Î¸,x) = âˆ‡Â²V(Î¸,x)[2,2]
âˆ‚âˆ‚Vpp(Î¸,x) = âˆ‡Â²V(Î¸,x)[3,3]
âˆ‚âˆ‚Vsp(Î¸,x) = âˆ‡Â²V(Î¸,x)[2,3]

#Value ingredients
Î”áµƒ(x,Î´áµƒ,ráµƒ) = [y(x) + (ráµƒ>=Î´áµƒ ? s(x)*(1+Î´áµƒ) : 0), s(x) + s(x)*ráµƒ, p(x) + s(x)*ráµƒ, q(x) + (ráµƒ>=Î´áµƒ ? -1 : 0), h(x)]
Î”áµ‡(x,Î´áµ‡,ráµ‡) = [y(x) + (ráµ‡>=Î´áµ‡ ? -s(x)*(1-Î´áµ‡) : 0), s(x) - s(x)*ráµ‡, p(x) - s(x)*ráµ‡, q(x) + (ráµ‡>=Î´áµ‡ ?  1 : 0), h(x)]
Î”áµƒV(Î¸,x,Î´áµƒ,ráµƒ) = V(Î¸,Î”áµƒ(x,Î´áµƒ,ráµƒ)) - V(Î¸,x)
Î”áµ‡V(Î¸,x,Î´áµ‡,ráµ‡) = V(Î¸,Î”áµ‡(x,Î´áµ‡,ráµ‡)) - V(Î¸,x)
Î”áµƒáµ–V(Î¸,x,ráµ–) = cheap(x) ? V(Î¸,[y(x),s(x),p(x)+s(x)*ráµ–,q(x),h(x)]) - V(Î¸,x) : 0
Î”áµ‡áµ–V(Î¸,x,ráµ–) = rich(x) ? V(Î¸,[y(x),s(x),p(x)-s(x)*ráµ–,q(x),h(x)]) - V(Î¸,x) : 0
EÎ”áµƒV(Î¸,x,Î´áµƒ) = mean([Î”áµƒV(Î¸,x,Î´áµƒ,r) for r in ráµƒ(x)])
EÎ”áµ‡V(Î¸,x,Î´áµ‡) = mean([Î”áµ‡V(Î¸,x,Î´áµ‡,r) for r in ráµ‡(x)])
EÎ”áµƒáµ–V(Î¸,x) = mean([Î”áµƒáµ–V(Î¸,x,r) for r in ráµƒ(x)])
EÎ”áµ‡áµ–V(Î¸,x) = mean([Î”áµ‡áµ–V(Î¸,x,r) for r in ráµ‡(x)])

#V(Î¸áµ›,xâ‚€), V(Î¸áµ›,Î”áµƒ(xâ‚€,0.0008,0.0008)), V(Î¸áµ›,Î”áµ‡(xâ‚€,0.0001,0.0001))

#Reward ingredients
Î”áµƒR(x,Î´áµƒ,ráµƒ) = R(Î”áµƒ(x,Î´áµƒ,ráµƒ)) - R(x)
Î”áµ‡R(x,Î´áµ‡,ráµ‡) = R(Î”áµ‡(x,Î´áµ‡,ráµ‡)) - R(x)
Î”áµƒáµ–R(x,ráµ–) = cheap(x) ? R([y(x),s(x),p(x)+s(x)*ráµ–,q(x),h(x)]) - R(x) : 0
Î”áµ‡áµ–R(x,ráµ–) = rich(x) ? R([y(x),s(x),p(x)-s(x)*ráµ–,q(x),h(x)]) - R(x) : 0
EÎ”áµƒR(x,Î´áµƒ) = mean([Î”áµƒR(x,Î´áµƒ,r) for r in ráµƒ(x)])
EÎ”áµ‡R(x,Î´áµ‡) = mean([Î”áµ‡R(x,Î´áµ‡,r) for r in ráµ‡(x)])
EÎ”áµƒáµ–R(x) = mean([Î”áµƒáµ–R(x,r) for r in ráµƒ(x)])
EÎ”áµ‡áµ–R(x) = mean([Î”áµ‡áµ–R(x,r) for r in ráµ‡(x)])

#Infinitesimal operator for V
function AV(Î¸,x,Î´áµƒ,Î´áµ‡)
    Î¸*(ğ”­*s(x)-p(x))*âˆ‚Vp(Î¸,x)
    + 0.5 * s(x)^2 *(Ï‚^2 * âˆ‚âˆ‚Vss(Î¸,x) + Îº^2 * âˆ‚âˆ‚Vpp(Î¸,x) + 2 * Ï± * Ï‚ * Îº * âˆ‚âˆ‚Vsp(Î¸,x))
    + Î»áµƒ^-1 * EÎ”áµƒV(Î¸,x,Î´áµƒ) + Î»áµ‡^-1 * EÎ”áµ‡V(Î¸,x,Î´áµ‡) + Î»áµ–^-1 * EÎ”áµƒáµ–V(Î¸,x) + Î»áµ–^-1 * EÎ”áµ‡áµ–V(Î¸,x)
end

#Infinitesimal operator for R
function AR(x,Î´áµƒ,Î´áµ‡)
    Parameters.Î¸*(ğ”­*s(x)-p(x))*âˆ‚Rp(x)
    + 0.5 * s(x)^2 *(Ï‚^2 * âˆ‚âˆ‚Rss(x) + Îº^2 * âˆ‚âˆ‚Rpp(x) + 2 * Ï± * Ï‚ * Îº * âˆ‚âˆ‚Rsp(x))
    + Î»áµƒ^-1 * EÎ”áµƒR(x,Î´áµƒ) + Î»áµ‡^-1 * EÎ”áµ‡R(x,Î´áµ‡) + Î»áµ–^-1 * EÎ”áµƒáµ–R(x) + Î»áµ–^-1 * EÎ”áµ‡áµ–R(x)
end

#L operator as described in the thesis
function LV(Î¸,x,Î´áµƒ,Î´áµ‡)
    AR(x,Î´áµƒ,Î´áµ‡) + AV(Î¸,x,Î´áµƒ,Î´áµ‡) - Ï*V(Î¸,x)
end

#Intervention function as described in the thesis
function Î“(x,Î¾)
    x .+ [-(s(x)-p(x))*Î¾ - abs(Î¾)*s(x)*Ï‡, 0, 0, 0, Î¾]
end

#Intervention operator
function MV(Î¸,x,Î¾)
    V(Î¸,Î“(x,Î¾))-V(Î¸,x)
end

#Establishes valid values for Î¾ depending on the position
function validÎ¾(q,h)
    if q+h> hedge_limit #long
        max(-q-h,-5):0
    elseif q+h < -hedge_limit
        0:min(-q-h,5)
    else
        0:0
    end
end

#Greedy controls
maxÎ´áµƒ(Î¸,x) = 0.0001*argmax([EÎ”áµƒV(Î¸,x,Î´)+EÎ”áµƒR(x,Î´) for Î´ in 0.0001:0.0001:0.0020])
maxÎ´áµ‡(Î¸,x) = 0.0001*argmax([EÎ”áµ‡V(Î¸,x,Î´)+EÎ”áµ‡R(x,Î´) for Î´ in 0.0001:0.0001:0.0020])
maxÎ¾(Î¸,x)  = validÎ¾(q(x),h(x))[argmax([V(Î¸,Î“(x,Î¾)) for Î¾ in validÎ¾(q(x),h(x))])]

function total_reward(xâ‚€;Î¸áµ›=Î¸áµ›,N=1000,T=1,Simulations=500,Safe=true)
    Î”t = 1/N
    reward = 0
    reward = @distributed (+) for i = 1:Simulations
        reward_i = 0
        sim = simulate(xâ‚€;N=N,T=T)
        t=1
        x = xâ‚€
        for j in sim
            bid = Safe ? min(0.01,max(0.0001,maxÎ´áµ‡(Î¸áµ›,x))) : maxÎ´áµ‡(Î¸áµ›,x)
            offer = Safe ? min(0.01,max(0.0001,maxÎ´áµƒ(Î¸áµ›,x))) : maxÎ´áµƒ(Î¸áµ›,x)
            hedge = maxÎ¾(Î¸áµ›,x)
            if isnan(hedge)
                hedge=0
            end
            if isinf(hedge)
                hedge=0
            end
            new_x = [y(x), j[1], j[2], q(x), h(x)]
            new_ráµƒ = j[3]
            new_ráµ‡ = j[4]
            if new_ráµƒ >= offer
                new_x = new_x + [s(new_x),0,0,-1,0]
            end
            if new_ráµ‡ >= bid
                new_x = new_x + [-s(new_x),0,0,1,0]
            end
            if offer < 0 || bid < 0
                new_x = new_x + [- s(new_x)*Ï‡,0,0,0,0]
            end
            posicao = q(x)+h(x)
            if posicao > hedge_limit
                hedge = max(-posicao,min(0,hedge))
            elseif posicao < -hedge_limit
                hedge = min(-posicao,max(0,hedge))
            end

            if abs(hedge) >= 1
                new_x = Î“(new_x,hedge)
            end
            reward_i += exp(-Ï*t*Î”t)*R(new_x) - exp(-Ï*(t-1)*Î”t)*R(x)
            if isnan(reward_i)
                println("NAN at ",x," -> ",new_x," hedge: ",hedge,", t: ",t)
                break
            end
            x = new_x
            t+=1
        end
        #reward += reward_i
        reward_i + exp(-Ï*t)*R(x)
    end
    reward/N
end


#Initial values for the function approximations parameters
Î¸áµ› = rand(length(Ï•(ones(5))))
Î¸áµƒ = zeros(length(Ï•(ones(5))))
Î¸áµ‡ = zeros(length(Ï•(ones(5))))
Î¸Ê· = zeros(length(Ï•(ones(5))))

V_estimate(Î¸,x) = max(LV(Î¸,x,maxÎ´áµƒ(Î¸,x),maxÎ´áµ‡(Î¸,x)),MV(Î¸,x,maxÎ¾(Î¸,x))) + v(x) #V(Î¸,x)
#V_estimate(Î¸,x) = V(Î¸,x) + max(max([LV(Î¸,x,Î´áµƒ,Î´áµ‡) for Î´áµƒ in 0.0001:0.0002:0.0016 for Î´áµ‡ in 0.0001:0.0002:0.0016]...), max([MV(Î¸,x,Î¾) for Î¾ in validÎ¾(q(x),h(x))]...))

 #max(LV(Î¸,x,maxÎ´áµƒ(Î¸,x),maxÎ´áµ‡(Î¸,x)),MV(Î¸,x,maxÎ¾(Î¸,x)))
V_estimate(Î¸) = vcat([V_estimate(Î¸,x) for x in batch]...)
V_bar(Î¸) = vcat([V(Î¸,x) for x in batch]...)
maxÎ´áµƒ(Î¸) = mean([maxÎ´áµƒ(Î¸,x) for x in batch])
maxÎ´áµ‡(Î¸) = mean([maxÎ´áµ‡(Î¸,x) for x in batch])
maxÎ¾(Î¸) = mean([maxÎ¾(Î¸,x) for x in batch])

#max( max([LV(Î¸,x,Î´áµƒ,Î´áµ‡) for Î´áµƒ in 0.0001:0.0001:0.0020 for Î´áµ‡ in 0.0001:0.0001:0.0020]...), max([MV(Î¸,x,Î¾) for Î¾ in validÎ¾(q(x),h(x))]...))

function second_order_condition(grad1,grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) for i in 1:5 for j in i:5 if i!=j]
end

function third_order_condition(grad1, grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] + grad1[k]*grad2[k] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) - abs(grad1[i]*grad2[k]+grad1[k]*grad2[i]) - abs(grad1[k]*grad2[j]+grad1[j]*grad2[k]) for i in 1:5 for j in i:5 for k in 1:5 if i<j && j<k]
end

function fourth_order_condition(grad1, grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] + grad1[k]*grad2[k] + grad1[h]*grad2[h] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) - abs(grad1[i]*grad2[k]+grad1[k]*grad2[i]) - abs(grad1[k]*grad2[j]+grad1[j]*grad2[k]) - abs(grad1[h]*grad2[i]+grad1[i]*grad2[h]) - abs(grad1[h]*grad2[j]+grad1[j]*grad2[h]) - abs(grad1[h]*grad2[k]+grad1[k]*grad2[h]) for i in 1:5 for j in i:5 for k in 1:5 for h in 1:5 if i<j && j<k && k<h]
end

function fifth_order_condition(grad1, grad2)
    [
    grad1[1]*grad2[1] +
    grad1[2]*grad2[2] +
    grad1[3]*grad2[3] +
    grad1[4]*grad2[4] +
    grad1[5]*grad2[5]
    - abs(grad1[1]*grad2[2]+grad1[2]*grad2[1])
    - abs(grad1[1]*grad2[3]+grad1[3]*grad2[1])
    - abs(grad1[3]*grad2[2]+grad1[2]*grad2[3])
    - abs(grad1[4]*grad2[1]+grad1[1]*grad2[4])
    - abs(grad1[4]*grad2[2]+grad1[2]*grad2[4])
    - abs(grad1[4]*grad2[3]+grad1[3]*grad2[4])
    - abs(grad1[5]*grad2[1]+grad1[1]*grad2[5])
    - abs(grad1[5]*grad2[2]+grad1[2]*grad2[5])
    - abs(grad1[5]*grad2[3]+grad1[3]*grad2[5])
    - abs(grad1[5]*grad2[4]+grad1[4]*grad2[5])
    ]
end

function V_constraint1(Î¸)
    vcat([âˆ‡Vx(Î¸,x) .* âˆ‡R(x) for x in batch]...)
end

function V_constraint2(Î¸)
    vcat([second_order_condition(âˆ‡Vx(Î¸,x),âˆ‡R(x)) for x in batch]...)
end

function V_constraint3(Î¸)
    vcat([third_order_condition(âˆ‡Vx(Î¸,x),âˆ‡R(x)) for x in batch]...)
end

function V_constraint4(Î¸)
    vcat([fourth_order_condition(âˆ‡Vx(Î¸,x),âˆ‡R(x)) for x in batch]...)
end

function V_constraint5(Î¸)
    vcat([fifth_order_condition(âˆ‡Vx(Î¸,x),âˆ‡R(x)) for x in batch]...)
end

function constraint(Î¸)
    [V_constraint1(Î¸);V_constraint2(Î¸);V_constraint3(Î¸);V_constraint4(Î¸);V_constraint5(Î¸)]
end

function objective_breakdown(z)
    Î¸ = z[1:length(Î¸áµ›)]
    Ïµ = z[length(Î¸)+1:length(Î¸)+41*length(batch)]
    Î» = z[length(Î¸)+1+41*length(batch):length(Î¸)+82*length(batch)]
    Î· = z[length(Î¸)+82*length(batch)+1]
    sum((V_estimate(Î¸)-V_bar(Î¸)).^2) , sum(abs.(Î¸)), sum(Î¸.^2), - Î»'*(constraint(Î¸) - Ïµ.^2) , 0.5 * Î· * sum((constraint(Î¸) - Ïµ.^2) .^2)
end
#Lagrangian objective
function objective(z)
    Î¸ = z[1:length(Î¸áµ›)]
    Ïµ = z[length(Î¸)+1:length(Î¸)+41*length(batch)]
    Î» = z[length(Î¸)+1+41*length(batch):length(Î¸)+82*length(batch)]
    Î· = z[length(Î¸)+82*length(batch)+1]
    sum((V_estimate(Î¸)-V_bar(Î¸)).^2) + 0.5sum(abs.(Î¸)) + 0.5sum(Î¸.^2) - Î»'*(constraint(Î¸) - Ïµ.^2) + 0.5 * Î· * sum((constraint(Î¸) - Ïµ.^2) .^2)
end
objective(Î¸,Ïµ,Î»,Î·) = objective([Î¸;Ïµ;Î»;Î·])
âˆ‡objective(Î¸,Ïµ,Î»,Î·) = ForwardDiff.gradient(objective,[Î¸;Ïµ;Î»;Î·])[1:length(Î¸)+41*length(batch)]

function minimize_lagrangian(Î¸â‚€,Î»;gradient_step = 10^-10, penalty_parameter = 1, n_iter = 40, debug = false)
    F = zeros(n_iter)
    Î¸ = [Î¸â‚€ for i in 1:n_iter]
    Ïµ = [zeros(length(Î»)) for i in 1:n_iter]
    improvement = [0.0 for i in 1:n_iter]
    restarted = false
    for iter = 1:n_iter
        F[iter] = objective(Î¸[iter],Ïµ[iter],Î»,penalty_parameter)
        if iter>1
            if F[iter]>F[iter-1]
                gradient_step /= 4
                restarted = true
                #restart the search
                println("> F[$iter] is worse than F[$(iter-1)]: $(F[iter]) > $(F[iter-1]), gradient_step was $gradient_step")
                println("> Restarting search")
                F[iter] = F[iter-1]
                Î¸[iter] = Î¸[iter-1]
                #return F[iter-1],Î¸[iter-1],Ïµ[iter-1],gradient_step
            elseif gradient_step < 10^-5 && !restarted
                gradient_step *= 2 #max(1,log(abs(F[iter]))/2)
            end
        end
        grad = âˆ‡objective(Î¸[iter],Ïµ[iter],Î»,penalty_parameter)
        if iter<n_iter
            Î¸[iter+1] = Î¸[iter] - gradient_step * grad[1:length(Î¸â‚€)]
            Ïµ[iter+1] = Ïµ[iter] - gradient_step * grad[length(Î¸â‚€)+1:end]
        elseif iter==n_iter
            return F[iter],Î¸[iter],Ïµ[iter],gradient_step
        end
        improvement[iter] = iter>1 ? max((F[iter-1]-F[iter])/abs(F[iter-1]), 0.0) : 0.0
        if improvement[iter] < 0.01 && !restarted
            gradient_step *= 10
        end
        if restarted && improvement[iter] < 2e-4 && improvement[iter] < improvement[iter-1]
            return F[iter],Î¸[iter],Ïµ[iter],gradient_step
        end
        if debug
            println("> Objective F(Î¸[$iter])): $(F[iter])", improvement[iter] > 0 ? ". Improvement: $(@sprintf("%.4f",100*improvement[iter]))%" : ". No improvemnt with gradient step $gradient_step.")
        end
    end
end

function solve_problem(Î¸â‚€;
    n_outer_iter = 20, n_inner_iter = 20, gradient_step = 10^-10, adjoint_step=10^-5,
    penalty_parameter=1, current_epoch = 1, debug=false)
    #Lagrange multipliers
    Î» = ones(41*length(batch))
    F = zeros(n_outer_iter)
    Î¸ = [Î¸â‚€ for i in 1:n_outer_iter]
    Ïµ = [ones(length(Î»)) for i in 1:n_outer_iter]
    Î· = penalty_parameter
    for iter = 1:n_outer_iter
        println("Iteration $current_epoch.$iter:")
        F[iter], Î¸[iter], Ïµ[iter], gradient_step = minimize_lagrangian(Î¸[iter],Î»;gradient_step=gradient_step,penalty_parameter=Î·,n_iter=n_inner_iter,debug=debug)
        Î» += adjoint_step * (constraint(Î¸[iter]) - Ïµ[iter].^2)
        Î» = [max(k,0) for k in Î»]
        println("Best F[$current_epoch.$iter] = $(F[iter])")
        if iter>1
            if F[iter] > F[iter-1]
                println("F[$current_epoch.$iter] is worse than F[$current_epoch.$(iter-1)]: $(F[iter]) > $(F[iter-1])")
                return Î¸[iter-1],gradient_step
            elseif iter==n_outer_iter
                return Î¸[iter],gradient_step
            elseif adjoint_step < 0.05
                adjoint_step *= 2
            end
        end
        Î¸[iter+1] = Î¸[iter]
    end
end

#The learning rates below indicate how much % we want to learn from the current best
#and how much we want to learn from the new optimized function, given the new data
learning_rate = 1.0

master_batch = randx(100)
master_batch_matrix = permutedims(Flux.batch(Ï•.(master_batch)))
master_mean = [abs(mean(master_batch_matrix[:,i])) for i in 1:length(Î¸áµ›)]

batch = sample(master_batch,4,replace=false)
Ï•_X = Ï•(batch)
Î¸áµ› = rand(length(Ï•(ones(5)))) - 0.5
penalty_parameter = 1 #2e-7
gradient_step = 10^-6
Î¸áµ›history = [Î¸áµ›]

q_coords = vcat([q for q in -20:1:20, h in -20:1:20]...)
h_coords = vcat([h for q in -20:1:20, h in -20:1:20]...)
bids = vcat([maxÎ´áµ‡(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
offers = vcat([maxÎ´áµƒ(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
v_values = vcat([V(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
surface(q_coords,h_coords,bids, title="Bid - Start")
surface(q_coords,h_coords,offers, title="Offer - Start")
surface(q_coords,h_coords,v_values, title="Value - Start")

for iter = 1:20
    println("Epoch: $iter")
    Î¸áµ›,gradient_step = solve_problem(Î¸áµ›,
        gradient_step = gradient_step,
        adjoint_step = 0.001,
        penalty_parameter = iter <= 10 ? penalty_parameter : 0.5*penalty_parameter,
        n_outer_iter = iter==1 ? 10 : 5,
        n_inner_iter = iter==1 ? 20 : 20,
        current_epoch = iter,
        debug = true)
    current_learning_rate = iter == 1 ? 1 : learning_rate*(1 - sqrt(iter)/4.9)
    global Î¸áµ› = last(Î¸áµ›history)*(1-current_learning_rate) + Î¸áµ› * current_learning_rate
    global Î¸áµ›history = [Î¸áµ›history..., Î¸áµ›]
    gradient_step /= 2
    println("V(xâ‚€) = ",V(Î¸áµ›,xâ‚€))
    println("Î¸[$iter] = $Î¸áµ›")
    bids = vcat([maxÎ´áµ‡(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    offers = vcat([maxÎ´áµƒ(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    v_values = vcat([V(Î¸áµ›,xâ‚€+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    surface(q_coords,h_coords,bids, title="Bid - $iter")
    surface(q_coords,h_coords,offers, title="Offer - $iter")
    surface(q_coords,h_coords,v_values, title="Value - $iter")
    #Replace half of the samples
    batch = sample(master_batch,4,replace=false)
    Ï•_X = Ï•(batch)
    #penalty_parameter *= 1/log(1+sum(Î¸áµ›.^2))
end

function generate_scenario(Î¸áµ›,f::Function, name::String, should_plot)
    scenario = DataFrame()
    scenario[:q] = vcat([y for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:h] = vcat([x for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:bid] = vcat([maxÎ´áµ‡(Î¸áµ›,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:offer] = vcat([maxÎ´áµƒ(Î¸áµ›,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:hedge] = vcat([maxÎ¾(Î¸áµ›,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:wealth] = vcat([v(f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:utility] = vcat([R(f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:value] = vcat([V(Î¸áµ›,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    if should_plot
        surface(scenario[:q],scenario[:h],scenario[:bid], title="Bid - $name")
        surface(scenario[:q],scenario[:h],scenario[:offer],  title="Offer - $name")
        surface(scenario[:q],scenario[:h],scenario[:hedge],  title="Hedge - $name")
        surface(scenario[:q],scenario[:h],scenario[:wealth], title="Wealth - $name")
        surface(scenario[:q],scenario[:h],scenario[:utility], title="Utility - $name")
        surface(scenario[:q],scenario[:h],scenario[:value], title="Value - $name")
    end
    return scenario
end

function generate_scenario2(Î¸áµ›,f::Function, name::String, should_plot)
    scenario = DataFrame()
    scenario[:q] = vcat([y for x in -50:2:50, y in -50:2:50]...)
    scenario[:h] = vcat([x for x in -50:2:50, y in -50:2:50]...)
    scenario[:bid] = vcat([maxÎ´áµ‡(Î¸áµ›,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    scenario[:offer] = vcat([maxÎ´áµƒ(Î¸áµ›,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:hedge] = vcat([maxÎ¾(Î¸áµ›,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:wealth] = vcat([v(f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:utility] = vcat([R(f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    scenario[:value] = vcat([V(Î¸áµ›,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    if should_plot
        surface(scenario[:q],scenario[:h],scenario[:bid], title="Bid - $name")
        surface(scenario[:q],scenario[:h],scenario[:offer],  title="Offer - $name")
        #surface(scenario[:q],scenario[:h],scenario[:hedge],  title="Hedge - $name")
        #surface(scenario[:q],scenario[:h],scenario[:wealth], title="Wealth - $name")
        #surface(scenario[:q],scenario[:h],scenario[:utility], title="Utility - $name")
        surface(scenario[:q],scenario[:h],scenario[:value], title="Value - $name")
    end
    return scenario
end

#generate_scenario2([-6459.41, 4817.17, 3239.14, 2914.76, 3803.45, -7771.34*0, 1182.54, 5644.38, -2739.32, -2762.34],(q,h) -> xâ‚€+[-100*(q+h),0,0,q,h], "Scenario 1 (*)", true)

#Same wealth scenario, different arrangements of position
generate_scenario(Î¸áµ›,(q,h) -> xâ‚€+[-100*(q+h),0,0,q,h], "Scenario 1", true)
#Zero cash scenarion, different arrangements of position
generate_scenario(Î¸áµ›,(q,h) -> xâ‚€+[0,0,0,q,h], "Scenario 2", true)



#random_direction = vcat(randx(1)...)
# random_x = sample(batch)
# sign.(âˆ‡R(random_x) .* âˆ‡V(Î¸áµ›,random_x)[1:5])
# sign.(âˆ‡R(random_x) .* âˆ‡V([0,1,0.00000001,0,0,0,0,0,0,-0.0001],random_x)[1:5])



#plot(-2000:100:2000,[R(xâ‚€+i*random_direction) for i in -2000:100:2000], title="R")
#plot(-2000:100:2000,[V(Î¸áµ›,xâ‚€+i*random_direction) for i in -2000:100:2000], title="V")
