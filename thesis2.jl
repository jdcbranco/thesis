#include("./Parameters.jl")
#include("./Simulator.jl")
if something(findfirst(isequal("./"),LOAD_PATH),0) == 0
    push!(LOAD_PATH, "./")
end
#using Distributions
using ForwardDiff
using JuMP
using NLopt
using GR
using DataFrames
using MultivariateStats
using GLMNet
using Flux:batch
using DelimitedFiles
using Statistics
using Parameters
using Simulator

#Basis functions
Ï•(x) = [v(x), y(x), s(x), p(x), q(x), h(x), s(x)^2, p(x)^2, q(x)^2, h(x)^2, log(s(x)),
        s(x)*q(x), h(x)*s(x), h(x)*p(x), tanh(v(x)/1000), tanh((q(x)+h(x))/1000)]

#Function approximations
V(Î¸,x) =  sum(Î¸ .* Ï•(x))
Î´áµƒ(Î¸,x) = sum(Î¸ .* Ï•(x))
Î´áµ‡(Î¸,x) = sum(Î¸ .* Ï•(x))
Î¾(Î¸,x) =  sum(Î¸ .* Ï•(x))

function get_param(X)
    x = X[1:5]
    Î¸ = X[6:length(X)]
    x,Î¸
end

function get_x(X)
    X[1:5]
end

#Value funcion as function of one master argument
#with both the state x and the parameters Î¸áµ›
function V(X)
    x,Î¸ = get_param(X)
    V(Î¸,x)
end

#Reward function partial derivatives
âˆ‡R(x) = ForwardDiff.gradient(R,x)
âˆ‡Â²R(x) = ForwardDiff.hessian(R,x)
âˆ‚Ry(x) = âˆ‡R(x)[1]
âˆ‚Rs(x) = âˆ‡R(x)[2]
âˆ‚Rp(x) = âˆ‡R(x)[3]
âˆ‚Rq(x) = âˆ‡R(x)[4]
âˆ‚Rh(x) = âˆ‡R(x)[5]
âˆ‚âˆ‚Rss(x) = âˆ‡Â²R(x)[2,2]
âˆ‚âˆ‚Rpp(x) = âˆ‡Â²R(x)[3,3]
âˆ‚âˆ‚Rsp(x) = âˆ‡Â²R(x)[2,3]

#Value function partial derivatives
âˆ‡V(Î¸,x) = ForwardDiff.gradient(V,[x;Î¸])
âˆ‡Vx(Î¸,x) = âˆ‡V(Î¸,x)[1:5]
âˆ‡Â²V(Î¸,x) = ForwardDiff.hessian(V,[x;Î¸])
âˆ‡Â²Vx(Î¸,x) = âˆ‡Â²V(Î¸,x)[1:5,1:5]
âˆ‚Vy(Î¸,x) = âˆ‡V(Î¸,x)[1]
âˆ‚Vs(Î¸,x) = âˆ‡V(Î¸,x)[2]
âˆ‚Vp(Î¸,x) = âˆ‡V(Î¸,x)[3]
âˆ‚Vq(Î¸,x) = âˆ‡V(Î¸,x)[4]
âˆ‚Vh(Î¸,x) = âˆ‡V(Î¸,x)[5]
âˆ‚âˆ‚Vss(Î¸,x) = âˆ‡Â²V(Î¸,x)[2,2]
âˆ‚âˆ‚Vpp(Î¸,x) = âˆ‡Â²V(Î¸,x)[3,3]
âˆ‚âˆ‚Vsp(Î¸,x) = âˆ‡Â²V(Î¸,x)[2,3]

#Value ingredients
Î”áµƒ(x,Î´áµƒ,ráµƒ) = [y(x) + (ráµƒ>=Î´áµƒ ? s(x)*(1+Î´áµƒ) : 0), s(x) + s(x)*ráµƒ, p(x) + s(x)*ráµƒ, q(x) + (ráµƒ>=Î´áµƒ ? -1 : 0), h(x)]
Î”áµ‡(x,Î´áµ‡,ráµ‡) = [y(x) + (ráµ‡>=Î´áµ‡ ? -s(x)*(1-Î´áµ‡) : 0), s(x) - s(x)*ráµ‡, p(x) - s(x)*ráµ‡, q(x) + (ráµ‡>=Î´áµ‡ ?  1 : 0), h(x)]
Î”áµƒV(Î¸,x,Î´áµƒ,ráµƒ) = V(Î¸,Î”áµƒ(x,Î´áµƒ,ráµƒ)) - V(Î¸,x)
Î”áµ‡V(Î¸,x,Î´áµ‡,ráµ‡) = V(Î¸,Î”áµ‡(x,Î´áµ‡,ráµ‡)) - V(Î¸,x)
Î”áµƒáµ–V(Î¸,x,ráµ–) = cheap(x) ? V(Î¸,[y(x),s(x),p(x)+s(x)*ráµ–,q(x),h(x)]) - V(Î¸,x) : 0
Î”áµ‡áµ–V(Î¸,x,ráµ–) = rich(x) ? V(Î¸,[y(x),s(x),p(x)-s(x)*ráµ–,q(x),h(x)]) - V(Î¸,x) : 0
EÎ”áµƒV(Î¸,x,Î´áµƒ) = mean([Î”áµƒV(Î¸,x,Î´áµƒ,r) for r in ráµƒ(x)])
EÎ”áµ‡V(Î¸,x,Î´áµ‡) = mean([Î”áµ‡V(Î¸,x,Î´áµ‡,r) for r in ráµ‡(x)])
EÎ”áµƒáµ–V(Î¸,x) = mean([Î”áµƒáµ–V(Î¸,x,r) for r in ráµƒ(x)])
EÎ”áµ‡áµ–V(Î¸,x) = mean([Î”áµ‡áµ–V(Î¸,x,r) for r in ráµ‡(x)])

#Reward ingredients
Î”áµƒR(x,Î´áµƒ,ráµƒ) = R(Î”áµƒ(x,Î´áµƒ,ráµƒ)) - R(x)
Î”áµ‡R(x,Î´áµ‡,ráµ‡) = R(Î”áµ‡(x,Î´áµ‡,ráµ‡)) - R(x)
Î”áµƒáµ–R(x,ráµ–) = cheap(x) ? R([y(x),s(x),p(x)+s(x)*ráµ–,q(x),h(x)]) - R(x) : 0
Î”áµ‡áµ–R(x,ráµ–) = rich(x) ? R([y(x),s(x),p(x)-s(x)*ráµ–,q(x),h(x)]) - R(x) : 0
EÎ”áµƒR(x,Î´áµƒ) = mean([Î”áµƒR(x,Î´áµƒ,r) for r in ráµƒ(x)])
EÎ”áµ‡R(x,Î´áµ‡) = mean([Î”áµ‡R(x,Î´áµ‡,r) for r in ráµ‡(x)])
EÎ”áµƒáµ–R(x) = mean([Î”áµƒáµ–R(x,r) for r in ráµƒ(x)])
EÎ”áµ‡áµ–R(x) = mean([Î”áµ‡áµ–R(x,r) for r in ráµ‡(x)])

#Infinitesimal operator for V
function AV(Î¸,x,Î´áµƒ,Î´áµ‡)
    Î¸*(ð”­*s(x)-p(x))*âˆ‚Vp(Î¸,x)
    + 0.5 * s(x)^2 *(Ï‚^2 * âˆ‚âˆ‚Vss(Î¸,x) + Îº^2 * âˆ‚âˆ‚Vpp(Î¸,x) + 2 * Ï± * Ï‚ * Îº * âˆ‚âˆ‚Vsp(Î¸,x))
    + Î»áµƒ^-1 * EÎ”áµƒV(Î¸,x,Î´áµƒ) + Î»áµ‡^-1 * EÎ”áµ‡V(Î¸,x,Î´áµ‡) + Î»áµ–^-1 * EÎ”áµƒáµ–V(Î¸,x) + Î»áµ–^-1 * EÎ”áµ‡áµ–V(Î¸,x)
end

#Infinitesimal operator for R
function AR(x,Î´áµƒ,Î´áµ‡)
    Parameters.Î¸*(ð”­*s(x)-p(x))*âˆ‚Rp(x)
    + 0.5 * s(x)^2 *(Ï‚^2 * âˆ‚âˆ‚Rss(x) + Îº^2 * âˆ‚âˆ‚Rpp(x) + 2 * Ï± * Ï‚ * Îº * âˆ‚âˆ‚Rsp(x))
    + Î»áµƒ^-1 * EÎ”áµƒR(x,Î´áµƒ) + Î»áµ‡^-1 * EÎ”áµ‡R(x,Î´áµ‡) + Î»áµ–^-1 * EÎ”áµƒáµ–R(x) + Î»áµ–^-1 * EÎ”áµ‡áµ–R(x)
end

#L operator as described in the thesis
function LV(Î¸,x,Î´áµƒ,Î´áµ‡)
    AR(x,Î´áµƒ,Î´áµ‡) + AV(Î¸,x,Î´áµƒ,Î´áµ‡) - Ï*V(Î¸,x)
end

#Intervention function as described in the thesis
function Î“(x,Î¾)
    x .+ [-(s(x)-p(x))*Î¾ - abs(Î¾)*s(x)*Ï‡, 0, 0, 0, Î¾]
end

#Intervention operator
function MV(Î¸,x,Î¾)
    V(Î¸,Î“(x,Î¾))-V(Î¸,x)
end

#Establishes valid values for Î¾ depending on the position
function validÎ¾(q,h)
    if q+h> hedge_limit #long
        max(-q-h,-5):0
    elseif q+h < -hedge_limit
        0:min(-q-h,5)
    else
        0:0
    end
end

#Greedy controls
maxÎ´áµƒ(Î¸,x) = 0.0001*argmax([EÎ”áµƒV(Î¸,x,Î´)+EÎ”áµƒR(x,Î´) for Î´ in 0.0001:0.0001:0.0020])
maxÎ´áµ‡(Î¸,x) = 0.0001*argmax([EÎ”áµ‡V(Î¸,x,Î´)+EÎ”áµ‡R(x,Î´) for Î´ in 0.0001:0.0001:0.0020])
maxÎ¾(Î¸,x)  = validÎ¾(q(x),h(x))[argmax([V(Î¸,Î“(x,Î¾)) for Î¾ in validÎ¾(q(x),h(x))])]

function V_estimate(Î¸,x)
    V(Î¸,x) + max(LV(Î¸,x,maxÎ´áµƒ(Î¸,x),maxÎ´áµ‡(Î¸,x)),MV(Î¸,x,maxÎ¾(Î¸,x)))
end

function total_reward(xâ‚€;Î¸áµƒ=Î¸áµƒ,Î¸áµ‡=Î¸áµ‡,Î¸Ê·=Î¸Ê·,N=1000,T=1,Simulations=500,Safe=true)
    Î”t = 1/N
    reward = 0
    reward = @parallel (+) for i = 1:Simulations
        reward_i = 0
        sim = simulate(xâ‚€;N=N,T=T)
        t=1
        x = xâ‚€
        for j in sim
            bid = Safe ? min(0.01,max(0.0001,Î´áµ‡(x))) : Î´áµ‡(Î¸áµ‡,x)
            offer = Safe ? min(0.01,max(0.0001,Î´áµƒ(x))) : Î´áµƒ(Î¸áµƒ,x)
            hedge = Î¾(Î¸Ê·,x)
            if isnan(hedge)
                hedge=0
            end
            if isinf(hedge)
                hedge=0
            end
            new_x = [y(x), j[1], j[2], q(x), h(x)]
            new_ráµƒ = j[3]
            new_ráµ‡ = j[4]
            if new_ráµƒ >= offer
                new_x = new_x + [s(new_x),0,0,-1,0]
            end
            if new_ráµ‡ >= bid
                new_x = new_x + [-s(new_x),0,0,1,0]
            end
            if offer < 0 || bid < 0
                new_x = new_x + [- s(new_x)*Ï‡,0,0,0,0]
            end
            posicao = q(x)+h(x)
            if posicao > hedge_limit
                hedge = max(-posicao,min(0,hedge))
            elseif posicao < -hedge_limit
                hedge = min(-posicao,max(0,hedge))
            end

            if abs(hedge) >= 1
                new_x = Î“(new_x,hedge)
            end
            reward_i += exp(-Ï*t*Î”t)*R(new_x) - exp(-Ï*(t-1)*Î”t)*R(x)
            if isnan(reward_i)
                println("NAN at ",x," -> ",new_x," hedge: ",hedge,", t: ",t)
                break
            end
            x = new_x
            t+=1
        end
        #reward += reward_i
        reward_i
    end
    reward/N
end


X_support = [randx(19;support=true);[xâ‚€]]
Ï•_X_support = Ï•.(X_support)
Ï•_X_data_support = permutedims(batch(Ï•_X_support))
V_support = R.(X_support)

X_sample = randx(100)
Ï•_X = Ï•.(X_sample)
Ï•_X_data = permutedims(batch(Ï•_X))

#The learning rates below indicate how much % we want to learn from the current best
#and how much we want to learn from the new optimized function, given the new data
learning_rate = 0.25
learning_best = 0.50

#Initial values for the function approximations parameters
Î¸áµ› = zeros(length(Ï•(ones(5))))
Î¸áµƒ = zeros(length(Ï•(ones(5))))
Î¸áµ‡ = zeros(length(Ï•(ones(5))))
Î¸Ê· = zeros(length(Ï•(ones(5))))

#The best reward is the best known expected total reward possible from state xâ‚€
#we also keep track of the policies that generated the best reward
if !@isdefined best_reward
    best_Î¸áµƒ = Î¸áµƒ
    best_Î¸áµ‡ = Î¸áµ‡
    best_Î¸Ê· = Î¸Ê·
    best_Î¸áµ› = Î¸áµ›
    best_reward = -Inf
end

function maxÎ´áµƒ(Î¸)
    mean([maxÎ´áµƒ(Î¸,x) for x in X_sample])
end

function maxÎ´áµ‡(Î¸)
    mean([maxÎ´áµ‡(Î¸,x) for x in X_sample])
end

function maxÎ¾(Î¸)
    mean([maxÎ¾(Î¸,x) for x in X_sample])
end

function V_estimate(Î¸)
    mean([V_estimate(Î¸,x) for x in X_sample])
end

function V_bar(Î¸)
    mean([V(Î¸,x) for x in X_sample])
end

function V_constraint1(Î¸)
    sum([âˆ‡Vx(Î¸,x) .* âˆ‡R(x) for x in X_sample])
end

function second_order_condition(grad1,grad2)
    [(grad1[i]-grad1[j])*(min(grad2[i]-grad2[j],grad2[i]+grad2[j])) for i in 1:5 for j in i:5 if i!=j]
end

function V_constraint2(Î¸)
    sum([second_order_condition(âˆ‡Vx(Î¸,x),âˆ‡R(x)) for x in X_sample])
end

#Lagrangian objective
function objective(z)
    Î¸ = z[1:length(Î¸áµ›)]
    Î»â‚ = z[length(Î¸áµ›)+1:length(Î¸áµ›)+5]
    Î»â‚‚ = z[length(Î¸áµ›)+6:length(Î¸áµ›)+15]
    (V_estimate(Î¸)-V_bar(Î¸))^2 - Î»â‚'*V_constraint1(Î¸) - Î»â‚‚'*V_constraint2(Î¸) + sum(z.^2) #this is a penalty/regularization
end
objective(Î¸,Î»â‚,Î»â‚‚) = objective([Î¸;Î»â‚;Î»â‚‚])
âˆ‡objective(Î¸,Î»â‚,Î»â‚‚) = ForwardDiff.gradient(objective,[Î¸;Î»â‚;Î»â‚‚])[1:length(Î¸áµ›)]

function minimize_lagrangian(Î¸â‚€,Î»â‚,Î»â‚‚;gradient_step = 0.000000002,n_iter = 40)
    F = zeros(n_iter)
    Î¸ = [Î¸â‚€ for x in 1:n_iter]
    for iter = 1:n_iter
        F[iter] = objective(Î¸[iter],Î»â‚,Î»â‚‚)
        if iter>1
            if F[iter]>F[iter-1]
                println("> F[$iter] is worse than F[$(iter-1)]: $(F[iter]) > $(F[iter-1])")
                return F[iter-1],Î¸[iter-1]
            elseif iter==n_iter
                return F[iter],Î¸[iter]
            else
                gradient_step *= 1.25
            end
        end
        Î¸[iter+1] = Î¸[iter] - gradient_step * âˆ‡objective(Î¸[iter],Î»â‚,Î»â‚‚)
        println("> Objective F(Î¸[$iter])): $(F[iter])")
    end
end
function solve_problem(Î¸â‚€;n_outer_iter = 5,n_inner_iter=5,gradient_step = 0.000000002,adjoint_step=0.000000002)
    #Lagrange multipliers
    Î»â‚ = zeros(5)
    Î»â‚‚ = zeros(10)
    F = zeros(n_outer_iter)
    Î¸ = [Î¸â‚€ for x in 1:n_outer_iter]
    for iter = 1:n_outer_iter
        println("Iteration $iter:")
        F[iter], Î¸[iter] = minimize_lagrangian(Î¸[iter],Î»â‚,Î»â‚‚;gradient_step=gradient_step,n_iter=n_inner_iter)
        Î»â‚ += adjoint_step * V_constraint1(Î¸[iter])
        Î»â‚‚ += adjoint_step * V_constraint2(Î¸[iter])
        println("Best F[$iter] = $(F[iter]) ")

        if iter>1
            if F[iter] > F[iter-1]
                println("F[$iter] is worse than F[$(iter-1)]: $(F[iter]) > $(F[iter-1])")
                return Î¸[iter-1]
            else iter==n_outer_iter
                return Î¸[iter]
            end
        end
        Î¸[iter+1] = Î¸[iter]
    end
    last(Î¸)
end

Î¸áµ› = zeros(length(Ï•(ones(5))))

Î¸áµ› = solve_problem(Î¸áµ›,gradient_step = 0.00000000002)
