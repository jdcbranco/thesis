if something(findfirst(isequal("./"),LOAD_PATH),0) == 0
    push!(LOAD_PATH, "./")
end
using Distributed
using Distributions
using ForwardDiff
using JuMP
using NLopt
using GR
using DataFrames
using MultivariateStats
using Flux
using DelimitedFiles
using Statistics
using Parameters
using Simulator

#Basis functions
œï(x) = [1, 1e-5*v(x),
        2.5e-9*(q(x)+h(x))*v(x),
        2.5e-3*s(x),
        1e-2*(s(x)-p(x)),
        #0.5e-3*(q(x)-h(x)),
        1e-5*q(x)*h(x),
        0.5e-3*(q(x)+h(x)),
        1e-7*(q(x)+h(x))^2,
        1e-7*(q(x)^2 + h(x)^2)]
œï(X::Array{Array{Float64,1},1}) = permutedims(Flux.batch(œï.(X)))

#Function approximations
V(Œ∏,x) =  sum(Œ∏ .* œï(x))
Œ¥·µÉ(Œ∏,x) = sum(Œ∏ .* œï(x))
Œ¥·µá(Œ∏,x) = sum(Œ∏ .* œï(x))
Œæ(Œ∏,x) =  sum(Œ∏ .* œï(x))

function get_param(X)
    x = X[1:5]
    Œ∏ = X[6:length(X)]
    x,Œ∏
end

function get_x(X)
    X[1:5]
end

#Value funcion as function of one master argument
#with both the state x and the parameters Œ∏·µõ
function V(X)
    x,Œ∏ = get_param(X)
    V(Œ∏,x)
end

#Reward function partial derivatives
‚àáR(x) = ForwardDiff.gradient(R,x)
‚àá¬≤R(x) = ForwardDiff.hessian(R,x)
‚àÇRy(x) = ‚àáR(x)[1]
‚àÇRs(x) = ‚àáR(x)[2]
‚àÇRp(x) = ‚àáR(x)[3]
‚àÇRq(x) = ‚àáR(x)[4]
‚àÇRh(x) = ‚àáR(x)[5]
‚àÇ‚àÇRss(x) = ‚àá¬≤R(x)[2,2]
‚àÇ‚àÇRpp(x) = ‚àá¬≤R(x)[3,3]
‚àÇ‚àÇRsp(x) = ‚àá¬≤R(x)[2,3]

#Value function partial derivatives
‚àáV(Œ∏,x) = ForwardDiff.gradient(V,[x;Œ∏])
‚àáVx(Œ∏,x) = ‚àáV(Œ∏,x)[1:5]
‚àá¬≤V(Œ∏,x) = ForwardDiff.hessian(V,[x;Œ∏])
‚àá¬≤Vx(Œ∏,x) = ‚àá¬≤V(Œ∏,x)[1:5,1:5]
‚àÇVy(Œ∏,x) = ‚àáV(Œ∏,x)[1]
‚àÇVs(Œ∏,x) = ‚àáV(Œ∏,x)[2]
‚àÇVp(Œ∏,x) = ‚àáV(Œ∏,x)[3]
‚àÇVq(Œ∏,x) = ‚àáV(Œ∏,x)[4]
‚àÇVh(Œ∏,x) = ‚àáV(Œ∏,x)[5]
‚àÇ‚àÇVss(Œ∏,x) = ‚àá¬≤V(Œ∏,x)[2,2]
‚àÇ‚àÇVpp(Œ∏,x) = ‚àá¬≤V(Œ∏,x)[3,3]
‚àÇ‚àÇVsp(Œ∏,x) = ‚àá¬≤V(Œ∏,x)[2,3]

#Value ingredients
Œî·µÉ(x,Œ¥·µÉ,r·µÉ) = [y(x) + (r·µÉ>=Œ¥·µÉ ? s(x)*(1+Œ¥·µÉ) : 0), s(x) + s(x)*r·µÉ, p(x) + s(x)*r·µÉ, q(x) + (r·µÉ>=Œ¥·µÉ ? -1 : 0), h(x)]
Œî·µá(x,Œ¥·µá,r·µá) = [y(x) + (r·µá>=Œ¥·µá ? -s(x)*(1-Œ¥·µá) : 0), s(x) - s(x)*r·µá, p(x) - s(x)*r·µá, q(x) + (r·µá>=Œ¥·µá ?  1 : 0), h(x)]
Œî·µÉV(Œ∏,x,Œ¥·µÉ,r·µÉ) = V(Œ∏,Œî·µÉ(x,Œ¥·µÉ,r·µÉ)) - V(Œ∏,x)
Œî·µáV(Œ∏,x,Œ¥·µá,r·µá) = V(Œ∏,Œî·µá(x,Œ¥·µá,r·µá)) - V(Œ∏,x)
Œî·µÉ·µñV(Œ∏,x,r·µñ) = cheap(x) ? V(Œ∏,[y(x),s(x),p(x)+s(x)*r·µñ,q(x),h(x)]) - V(Œ∏,x) : 0
Œî·µá·µñV(Œ∏,x,r·µñ) = rich(x) ? V(Œ∏,[y(x),s(x),p(x)-s(x)*r·µñ,q(x),h(x)]) - V(Œ∏,x) : 0
EŒî·µÉV(Œ∏,x,Œ¥·µÉ) = mean([Œî·µÉV(Œ∏,x,Œ¥·µÉ,r) for r in r·µÉ(x)])
EŒî·µáV(Œ∏,x,Œ¥·µá) = mean([Œî·µáV(Œ∏,x,Œ¥·µá,r) for r in r·µá(x)])
EŒî·µÉ·µñV(Œ∏,x) = mean([Œî·µÉ·µñV(Œ∏,x,r) for r in r·µÉ(x)])
EŒî·µá·µñV(Œ∏,x) = mean([Œî·µá·µñV(Œ∏,x,r) for r in r·µá(x)])

#V(Œ∏·µõ,x‚ÇÄ), V(Œ∏·µõ,Œî·µÉ(x‚ÇÄ,0.0008,0.0008)), V(Œ∏·µõ,Œî·µá(x‚ÇÄ,0.0001,0.0001))

#Reward ingredients
Œî·µÉR(x,Œ¥·µÉ,r·µÉ) = R(Œî·µÉ(x,Œ¥·µÉ,r·µÉ)) - R(x)
Œî·µáR(x,Œ¥·µá,r·µá) = R(Œî·µá(x,Œ¥·µá,r·µá)) - R(x)
Œî·µÉ·µñR(x,r·µñ) = cheap(x) ? R([y(x),s(x),p(x)+s(x)*r·µñ,q(x),h(x)]) - R(x) : 0
Œî·µá·µñR(x,r·µñ) = rich(x) ? R([y(x),s(x),p(x)-s(x)*r·µñ,q(x),h(x)]) - R(x) : 0
EŒî·µÉR(x,Œ¥·µÉ) = mean([Œî·µÉR(x,Œ¥·µÉ,r) for r in r·µÉ(x)])
EŒî·µáR(x,Œ¥·µá) = mean([Œî·µáR(x,Œ¥·µá,r) for r in r·µá(x)])
EŒî·µÉ·µñR(x) = mean([Œî·µÉ·µñR(x,r) for r in r·µÉ(x)])
EŒî·µá·µñR(x) = mean([Œî·µá·µñR(x,r) for r in r·µá(x)])

#Infinitesimal operator for V
function AV(Œ∏,x,Œ¥·µÉ,Œ¥·µá)
    Œ∏*(ùî≠*s(x)-p(x))*‚àÇVp(Œ∏,x)
    + 0.5 * s(x)^2 *(œÇ^2 * ‚àÇ‚àÇVss(Œ∏,x) + Œ∫^2 * ‚àÇ‚àÇVpp(Œ∏,x) + 2 * œ± * œÇ * Œ∫ * ‚àÇ‚àÇVsp(Œ∏,x))
    + Œª·µÉ^-1 * EŒî·µÉV(Œ∏,x,Œ¥·µÉ) + Œª·µá^-1 * EŒî·µáV(Œ∏,x,Œ¥·µá) + Œª·µñ^-1 * EŒî·µÉ·µñV(Œ∏,x) + Œª·µñ^-1 * EŒî·µá·µñV(Œ∏,x)
end

#Infinitesimal operator for R
function AR(x,Œ¥·µÉ,Œ¥·µá)
    Parameters.Œ∏*(ùî≠*s(x)-p(x))*‚àÇRp(x)
    + 0.5 * s(x)^2 *(œÇ^2 * ‚àÇ‚àÇRss(x) + Œ∫^2 * ‚àÇ‚àÇRpp(x) + 2 * œ± * œÇ * Œ∫ * ‚àÇ‚àÇRsp(x))
    + Œª·µÉ^-1 * EŒî·µÉR(x,Œ¥·µÉ) + Œª·µá^-1 * EŒî·µáR(x,Œ¥·µá) + Œª·µñ^-1 * EŒî·µÉ·µñR(x) + Œª·µñ^-1 * EŒî·µá·µñR(x)
end

#L operator as described in the thesis
function LV(Œ∏,x,Œ¥·µÉ,Œ¥·µá)
    AR(x,Œ¥·µÉ,Œ¥·µá) + AV(Œ∏,x,Œ¥·µÉ,Œ¥·µá) - œÅ*V(Œ∏,x)
end

#Intervention function as described in the thesis
function Œì(x,Œæ)
    x .+ [-(s(x)-p(x))*Œæ - abs(Œæ)*s(x)*œá, 0, 0, 0, Œæ]
end

#Intervention operator
function MV(Œ∏,x,Œæ)
    V(Œ∏,Œì(x,Œæ))-V(Œ∏,x)
end

#Establishes valid values for Œæ depending on the position
function validŒæ(q,h)
    if q+h> hedge_limit #long
        max(-q-h,-5):0
    elseif q+h < -hedge_limit
        0:min(-q-h,5)
    else
        0:0
    end
end

#Greedy controls
maxŒ¥·µÉ(Œ∏,x) = 0.0001*argmax([EŒî·µÉV(Œ∏,x,Œ¥)+EŒî·µÉR(x,Œ¥) for Œ¥ in 0.0001:0.0001:0.0020])
maxŒ¥·µá(Œ∏,x) = 0.0001*argmax([EŒî·µáV(Œ∏,x,Œ¥)+EŒî·µáR(x,Œ¥) for Œ¥ in 0.0001:0.0001:0.0020])
maxŒæ(Œ∏,x)  = validŒæ(q(x),h(x))[argmax([V(Œ∏,Œì(x,Œæ)) for Œæ in validŒæ(q(x),h(x))])]

function total_reward(x‚ÇÄ;Œ∏·µõ=Œ∏·µõ,N=1000,T=1,Simulations=500,Safe=true)
    Œît = 1/N
    reward = 0
    reward = @distributed (+) for i = 1:Simulations
        reward_i = 0
        sim = simulate(x‚ÇÄ;N=N,T=T)
        t=1
        x = x‚ÇÄ
        for j in sim
            bid = Safe ? min(0.01,max(0.0001,maxŒ¥·µá(Œ∏·µõ,x))) : maxŒ¥·µá(Œ∏·µõ,x)
            offer = Safe ? min(0.01,max(0.0001,maxŒ¥·µÉ(Œ∏·µõ,x))) : maxŒ¥·µÉ(Œ∏·µõ,x)
            hedge = maxŒæ(Œ∏·µõ,x)
            if isnan(hedge)
                hedge=0
            end
            if isinf(hedge)
                hedge=0
            end
            new_x = [y(x), j[1], j[2], q(x), h(x)]
            new_r·µÉ = j[3]
            new_r·µá = j[4]
            if new_r·µÉ >= offer
                new_x = new_x + [s(new_x),0,0,-1,0]
            end
            if new_r·µá >= bid
                new_x = new_x + [-s(new_x),0,0,1,0]
            end
            if offer < 0 || bid < 0
                new_x = new_x + [- s(new_x)*œá,0,0,0,0]
            end
            posicao = q(x)+h(x)
            if posicao > hedge_limit
                hedge = max(-posicao,min(0,hedge))
            elseif posicao < -hedge_limit
                hedge = min(-posicao,max(0,hedge))
            end

            if abs(hedge) >= 1
                new_x = Œì(new_x,hedge)
            end
            reward_i += exp(-œÅ*t*Œît)*R(new_x) - exp(-œÅ*(t-1)*Œît)*R(x)
            if isnan(reward_i)
                println("NAN at ",x," -> ",new_x," hedge: ",hedge,", t: ",t)
                break
            end
            x = new_x
            t+=1
        end
        #reward += reward_i
        reward_i + exp(-œÅ*t)*R(x)
    end
    reward/N
end


#Initial values for the function approximations parameters
Œ∏·µõ = rand(length(œï(ones(5))))
Œ∏·µÉ = zeros(length(œï(ones(5))))
Œ∏·µá = zeros(length(œï(ones(5))))
Œ∏ ∑ = zeros(length(œï(ones(5))))

V_estimate(Œ∏,x) = max(LV(Œ∏,x,maxŒ¥·µÉ(Œ∏,x),maxŒ¥·µá(Œ∏,x)),MV(Œ∏,x,maxŒæ(Œ∏,x))) + v(x) #V(Œ∏,x)
#V_estimate(Œ∏,x) = V(Œ∏,x) + max(max([LV(Œ∏,x,Œ¥·µÉ,Œ¥·µá) for Œ¥·µÉ in 0.0001:0.0002:0.0016 for Œ¥·µá in 0.0001:0.0002:0.0016]...), max([MV(Œ∏,x,Œæ) for Œæ in validŒæ(q(x),h(x))]...))

 #max(LV(Œ∏,x,maxŒ¥·µÉ(Œ∏,x),maxŒ¥·µá(Œ∏,x)),MV(Œ∏,x,maxŒæ(Œ∏,x)))
V_estimate(Œ∏) = vcat([V_estimate(Œ∏,x) for x in batch]...)
V_bar(Œ∏) = vcat([V(Œ∏,x) for x in batch]...)
maxŒ¥·µÉ(Œ∏) = mean([maxŒ¥·µÉ(Œ∏,x) for x in batch])
maxŒ¥·µá(Œ∏) = mean([maxŒ¥·µá(Œ∏,x) for x in batch])
maxŒæ(Œ∏) = mean([maxŒæ(Œ∏,x) for x in batch])

#max( max([LV(Œ∏,x,Œ¥·µÉ,Œ¥·µá) for Œ¥·µÉ in 0.0001:0.0001:0.0020 for Œ¥·µá in 0.0001:0.0001:0.0020]...), max([MV(Œ∏,x,Œæ) for Œæ in validŒæ(q(x),h(x))]...))

function second_order_condition(grad1,grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) for i in 1:5 for j in i:5 if i!=j]
end

function third_order_condition(grad1, grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] + grad1[k]*grad2[k] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) - abs(grad1[i]*grad2[k]+grad1[k]*grad2[i]) - abs(grad1[k]*grad2[j]+grad1[j]*grad2[k]) for i in 1:5 for j in i:5 for k in 1:5 if i<j && j<k]
end

function fourth_order_condition(grad1, grad2)
    [grad1[i]*grad2[i] + grad1[j]*grad2[j] + grad1[k]*grad2[k] + grad1[h]*grad2[h] - abs(grad1[i]*grad2[j]+grad1[j]*grad2[i]) - abs(grad1[i]*grad2[k]+grad1[k]*grad2[i]) - abs(grad1[k]*grad2[j]+grad1[j]*grad2[k]) - abs(grad1[h]*grad2[i]+grad1[i]*grad2[h]) - abs(grad1[h]*grad2[j]+grad1[j]*grad2[h]) - abs(grad1[h]*grad2[k]+grad1[k]*grad2[h]) for i in 1:5 for j in i:5 for k in 1:5 for h in 1:5 if i<j && j<k && k<h]
end

function fifth_order_condition(grad1, grad2)
    [
    grad1[1]*grad2[1] +
    grad1[2]*grad2[2] +
    grad1[3]*grad2[3] +
    grad1[4]*grad2[4] +
    grad1[5]*grad2[5]
    - abs(grad1[1]*grad2[2]+grad1[2]*grad2[1])
    - abs(grad1[1]*grad2[3]+grad1[3]*grad2[1])
    - abs(grad1[3]*grad2[2]+grad1[2]*grad2[3])
    - abs(grad1[4]*grad2[1]+grad1[1]*grad2[4])
    - abs(grad1[4]*grad2[2]+grad1[2]*grad2[4])
    - abs(grad1[4]*grad2[3]+grad1[3]*grad2[4])
    - abs(grad1[5]*grad2[1]+grad1[1]*grad2[5])
    - abs(grad1[5]*grad2[2]+grad1[2]*grad2[5])
    - abs(grad1[5]*grad2[3]+grad1[3]*grad2[5])
    - abs(grad1[5]*grad2[4]+grad1[4]*grad2[5])
    ]
end

function V_constraint1(Œ∏)
    vcat([‚àáVx(Œ∏,x) .* ‚àáR(x) for x in batch]...)
end

function V_constraint2(Œ∏)
    vcat([second_order_condition(‚àáVx(Œ∏,x),‚àáR(x)) for x in batch]...)
end

function V_constraint3(Œ∏)
    vcat([third_order_condition(‚àáVx(Œ∏,x),‚àáR(x)) for x in batch]...)
end

function V_constraint4(Œ∏)
    vcat([fourth_order_condition(‚àáVx(Œ∏,x),‚àáR(x)) for x in batch]...)
end

function V_constraint5(Œ∏)
    vcat([fifth_order_condition(‚àáVx(Œ∏,x),‚àáR(x)) for x in batch]...)
end

function constraint(Œ∏)
    [V_constraint1(Œ∏);V_constraint2(Œ∏);V_constraint3(Œ∏);V_constraint4(Œ∏);V_constraint5(Œ∏)]
end

function objective_breakdown(z)
    Œ∏ = z[1:length(Œ∏·µõ)]
    œµ = z[length(Œ∏)+1:length(Œ∏)+41*length(batch)]
    Œª = z[length(Œ∏)+1+41*length(batch):length(Œ∏)+82*length(batch)]
    Œ∑ = z[length(Œ∏)+82*length(batch)+1]
    sum((V_estimate(Œ∏)-V_bar(Œ∏)).^2) , sum(abs.(Œ∏)), sum(Œ∏.^2), - Œª'*(constraint(Œ∏) - œµ.^2) , 0.5 * Œ∑ * sum((constraint(Œ∏) - œµ.^2) .^2)
end
#Lagrangian objective
function objective(z)
    Œ∏ = z[1:length(Œ∏·µõ)]
    œµ = z[length(Œ∏)+1:length(Œ∏)+41*length(batch)]
    Œª = z[length(Œ∏)+1+41*length(batch):length(Œ∏)+82*length(batch)]
    Œ∑ = z[length(Œ∏)+82*length(batch)+1]
    sum((V_estimate(Œ∏)-V_bar(Œ∏)).^2) + 0.5sum(abs.(Œ∏)) + 0.5sum(Œ∏.^2) - Œª'*(constraint(Œ∏) - œµ.^2) + 0.5 * Œ∑ * sum((constraint(Œ∏) - œµ.^2) .^2)
end
objective(Œ∏,œµ,Œª,Œ∑) = objective([Œ∏;œµ;Œª;Œ∑])
‚àáobjective(Œ∏,œµ,Œª,Œ∑) = ForwardDiff.gradient(objective,[Œ∏;œµ;Œª;Œ∑])[1:length(Œ∏)+41*length(batch)]

function minimize_lagrangian(Œ∏‚ÇÄ,Œª;gradient_step = 10^-10, penalty_parameter = 1, n_iter = 40, debug = false)
    F = zeros(n_iter)
    Œ∏ = [Œ∏‚ÇÄ for i in 1:n_iter]
    œµ = [zeros(length(Œª)) for i in 1:n_iter]
    improvement = [0.0 for i in 1:n_iter]
    restarted = false
    for iter = 1:n_iter
        F[iter] = objective(Œ∏[iter],œµ[iter],Œª,penalty_parameter)
        if iter>1
            if F[iter]>F[iter-1]
                gradient_step /= 4
                restarted = true
                #restart the search
                println("> F[$iter] is worse than F[$(iter-1)]: $(F[iter]) > $(F[iter-1]), gradient_step was $gradient_step")
                println("> Restarting search")
                F[iter] = F[iter-1]
                Œ∏[iter] = Œ∏[iter-1]
                #return F[iter-1],Œ∏[iter-1],œµ[iter-1],gradient_step
            elseif gradient_step < 10^-5 && !restarted
                gradient_step *= 2 #max(1,log(abs(F[iter]))/2)
            end
        end
        grad = ‚àáobjective(Œ∏[iter],œµ[iter],Œª,penalty_parameter)
        if iter<n_iter
            Œ∏[iter+1] = Œ∏[iter] - gradient_step * grad[1:length(Œ∏‚ÇÄ)]
            œµ[iter+1] = œµ[iter] - gradient_step * grad[length(Œ∏‚ÇÄ)+1:end]
        elseif iter==n_iter
            return F[iter],Œ∏[iter],œµ[iter],gradient_step
        end
        improvement[iter] = iter>1 ? max((F[iter-1]-F[iter])/abs(F[iter-1]), 0.0) : 0.0
        if improvement[iter] < 0.01 && !restarted
            gradient_step *= 10
        end
        if restarted && improvement[iter] < 2e-4 && improvement[iter] < improvement[iter-1]
            return F[iter],Œ∏[iter],œµ[iter],gradient_step
        end
        if debug
            println("> Objective F(Œ∏[$iter])): $(F[iter])", improvement[iter] > 0 ? ". Improvement: $(@sprintf("%.4f",100*improvement[iter]))%" : ". No improvemnt with gradient step $gradient_step.")
        end
    end
end

function solve_problem(Œ∏‚ÇÄ;
    n_outer_iter = 20, n_inner_iter = 20, gradient_step = 10^-10, adjoint_step=10^-5,
    penalty_parameter=1, current_epoch = 1, debug=false)
    #Lagrange multipliers
    Œª = ones(41*length(batch))
    F = zeros(n_outer_iter)
    Œ∏ = [Œ∏‚ÇÄ for i in 1:n_outer_iter]
    œµ = [ones(length(Œª)) for i in 1:n_outer_iter]
    Œ∑ = penalty_parameter
    for iter = 1:n_outer_iter
        println("Iteration $current_epoch.$iter:")
        F[iter], Œ∏[iter], œµ[iter], gradient_step = minimize_lagrangian(Œ∏[iter],Œª;gradient_step=gradient_step,penalty_parameter=Œ∑,n_iter=n_inner_iter,debug=debug)
        Œª += adjoint_step * (constraint(Œ∏[iter]) - œµ[iter].^2)
        Œª = [max(k,0) for k in Œª]
        println("Best F[$current_epoch.$iter] = $(F[iter])")
        if iter>1
            if F[iter] > F[iter-1]
                println("F[$current_epoch.$iter] is worse than F[$current_epoch.$(iter-1)]: $(F[iter]) > $(F[iter-1])")
                return Œ∏[iter-1],gradient_step
            elseif iter==n_outer_iter
                return Œ∏[iter],gradient_step
            elseif adjoint_step < 0.05
                adjoint_step *= 2
            end
        end
        Œ∏[iter+1] = Œ∏[iter]
    end
end

#The learning rates below indicate how much % we want to learn from the current best
#and how much we want to learn from the new optimized function, given the new data
learning_rate = 1.0

master_batch = randx(100)
master_batch_matrix = permutedims(Flux.batch(œï.(master_batch)))
master_mean = [abs(mean(master_batch_matrix[:,i])) for i in 1:length(Œ∏·µõ)]

batch = sample(master_batch,4,replace=false)
œï_X = œï(batch)
Œ∏·µõ = rand(length(œï(ones(5)))) - 0.5
penalty_parameter = 1 #2e-7
gradient_step = 10^-6
Œ∏·µõhistory = [Œ∏·µõ]

q_coords = vcat([q for q in -20:1:20, h in -20:1:20]...)
h_coords = vcat([h for q in -20:1:20, h in -20:1:20]...)
bids = vcat([maxŒ¥·µá(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
offers = vcat([maxŒ¥·µÉ(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
v_values = vcat([V(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
surface(q_coords,h_coords,bids, title="Bid - Start")
surface(q_coords,h_coords,offers, title="Offer - Start")
surface(q_coords,h_coords,v_values, title="Value - Start")

for iter = 1:20
    println("Epoch: $iter")
    Œ∏·µõ,gradient_step = solve_problem(Œ∏·µõ,
        gradient_step = gradient_step,
        adjoint_step = 0.001,
        penalty_parameter = iter <= 10 ? penalty_parameter : 0.5*penalty_parameter,
        n_outer_iter = iter==1 ? 10 : 5,
        n_inner_iter = iter==1 ? 20 : 20,
        current_epoch = iter,
        debug = true)
    current_learning_rate = iter == 1 ? 1 : learning_rate*(1 - sqrt(iter)/4.9)
    global Œ∏·µõ = last(Œ∏·µõhistory)*(1-current_learning_rate) + Œ∏·µõ * current_learning_rate
    global Œ∏·µõhistory = [Œ∏·µõhistory..., Œ∏·µõ]
    gradient_step /= 2
    println("V(x‚ÇÄ) = ",V(Œ∏·µõ,x‚ÇÄ))
    println("Œ∏[$iter] = $Œ∏·µõ")
    bids = vcat([maxŒ¥·µá(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    offers = vcat([maxŒ¥·µÉ(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    v_values = vcat([V(Œ∏·µõ,x‚ÇÄ+[-100*(q+h),0,0,q,h]) for q in -20:1:20, h in -20:1:20]...)
    surface(q_coords,h_coords,bids, title="Bid - $iter")
    surface(q_coords,h_coords,offers, title="Offer - $iter")
    surface(q_coords,h_coords,v_values, title="Value - $iter")
    #Replace half of the samples
    batch = sample(master_batch,4,replace=false)
    œï_X = œï(batch)
    #penalty_parameter *= 1/log(1+sum(Œ∏·µõ.^2))
end

function generate_scenario(Œ∏·µõ,f::Function, name::String, should_plot)
    scenario = DataFrame()
    scenario[:q] = vcat([y for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:h] = vcat([x for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:bid] = vcat([maxŒ¥·µá(Œ∏·µõ,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:offer] = vcat([maxŒ¥·µÉ(Œ∏·µõ,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:hedge] = vcat([maxŒæ(Œ∏·µõ,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:wealth] = vcat([v(f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:utility] = vcat([R(f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    scenario[:value] = vcat([V(Œ∏·µõ,f(y,x)) for x in -2000:100:2000, y in -2000:100:2000]...)
    if should_plot
        surface(scenario[:q],scenario[:h],scenario[:bid], title="Bid - $name")
        surface(scenario[:q],scenario[:h],scenario[:offer],  title="Offer - $name")
        surface(scenario[:q],scenario[:h],scenario[:hedge],  title="Hedge - $name")
        surface(scenario[:q],scenario[:h],scenario[:wealth], title="Wealth - $name")
        surface(scenario[:q],scenario[:h],scenario[:utility], title="Utility - $name")
        surface(scenario[:q],scenario[:h],scenario[:value], title="Value - $name")
    end
    return scenario
end

function generate_scenario2(Œ∏·µõ,f::Function, name::String, should_plot)
    scenario = DataFrame()
    scenario[:q] = vcat([y for x in -50:2:50, y in -50:2:50]...)
    scenario[:h] = vcat([x for x in -50:2:50, y in -50:2:50]...)
    scenario[:bid] = vcat([maxŒ¥·µá(Œ∏·µõ,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    scenario[:offer] = vcat([maxŒ¥·µÉ(Œ∏·µõ,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:hedge] = vcat([maxŒæ(Œ∏·µõ,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:wealth] = vcat([v(f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    #scenario[:utility] = vcat([R(f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    scenario[:value] = vcat([V(Œ∏·µõ,f(y,x)) for x in -50:2:50, y in -50:2:50]...)
    if should_plot
        surface(scenario[:q],scenario[:h],scenario[:bid], title="Bid - $name")
        surface(scenario[:q],scenario[:h],scenario[:offer],  title="Offer - $name")
        #surface(scenario[:q],scenario[:h],scenario[:hedge],  title="Hedge - $name")
        #surface(scenario[:q],scenario[:h],scenario[:wealth], title="Wealth - $name")
        #surface(scenario[:q],scenario[:h],scenario[:utility], title="Utility - $name")
        surface(scenario[:q],scenario[:h],scenario[:value], title="Value - $name")
    end
    return scenario
end

#generate_scenario2([-6459.41, 4817.17, 3239.14, 2914.76, 3803.45, -7771.34*0, 1182.54, 5644.38, -2739.32, -2762.34],(q,h) -> x‚ÇÄ+[-100*(q+h),0,0,q,h], "Scenario 1 (*)", true)

#Same wealth scenario, different arrangements of position
generate_scenario(Œ∏·µõ,(q,h) -> x‚ÇÄ+[-100*(q+h),0,0,q,h], "Scenario 1", true)
#Zero cash scenarion, different arrangements of position
generate_scenario(Œ∏·µõ,(q,h) -> x‚ÇÄ+[0,0,0,q,h], "Scenario 2", true)



#random_direction = vcat(randx(1)...)
# random_x = sample(batch)
# sign.(‚àáR(random_x) .* ‚àáV(Œ∏·µõ,random_x)[1:5])
# sign.(‚àáR(random_x) .* ‚àáV([0,1,0.00000001,0,0,0,0,0,0,-0.0001],random_x)[1:5])



#plot(-2000:100:2000,[R(x‚ÇÄ+i*random_direction) for i in -2000:100:2000], title="R")
#plot(-2000:100:2000,[V(Œ∏·µõ,x‚ÇÄ+i*random_direction) for i in -2000:100:2000], title="V")
